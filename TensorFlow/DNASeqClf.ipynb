{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on Splice-junction Gene Sequences \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "MY_MODE_TRAINING = True\n",
    "MY_MODE_PREDICTION = False\n",
    "LOGDIR = './TMP/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def load_data(file_path) :\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    df.columns = ['classlabel', 'name', 'sequence']\n",
    "    df.tail()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the dataset\n",
    "* Apply one-hot-encoding to input data\n",
    "* Take 20% as test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df) :\n",
    "    \n",
    "    # Encoding class labels\n",
    "    class_le = LabelEncoder()\n",
    "    y = class_le.fit_transform(df['classlabel'].values)\n",
    "    #print(\"y:\",y)\n",
    "    \n",
    "    # Encoding sequence\n",
    "    # Here we use one hot encoding to encode the character in DNA sequence. \n",
    "    # So each dna sequence is converted to a 60x8 2D array \n",
    "    def Seq2Vec(seq):\n",
    "        s = str(seq).strip()\n",
    "        CharDict = { \"A\":[0,0,0,0,0,0,0,1],\n",
    "                     \"G\":[0,0,0,0,0,0,1,0],\n",
    "                     \"C\":[0,0,0,0,0,1,0,0],\n",
    "                     \"T\":[0,0,0,0,1,0,0,0],\n",
    "                     \"D\":[0,0,0,1,0,0,0,0],\n",
    "                     \"N\":[0,0,1,0,0,0,0,0],\n",
    "                     \"S\":[0,1,0,0,0,0,0,0],\n",
    "                     \"R\":[1,0,0,0,0,0,0,0]}\n",
    "        return np.asarray([CharDict[c] for c in s], dtype=np.float32).flatten()\n",
    "\n",
    "    df['seqvec'] = df['sequence'].apply(Seq2Vec)\n",
    "    X = np.vstack(df['seqvec'].values)\n",
    "    print(\"Total samples:\", X.shape[0])\n",
    "    \n",
    "    # Split the data set into training/test set\n",
    "    sss = StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=0)\n",
    "    for train_index, test_index in sss.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    print(\"Training samples: \", X_train.shape[0], \"Test samples: \", X_test.shape[0])\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next batch - support function\n",
    "* Get the next mini_batch from training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels, epoch, rounds):\n",
    "    ''' \n",
    "    Return a total of `num` random samples and labels. \n",
    "    Reshuffle the index when running over the total data set epoch%rounds==0\n",
    "    \n",
    "    Arg: \n",
    "      num, the number of returned data size\n",
    "      data, the input X\n",
    "      labels, the label y\n",
    "      epoch, the current epoch value\n",
    "      rounds, rounds = label_size // num \n",
    "\n",
    "    Returns:\n",
    "      Return \"num\" of X and y array\n",
    "    '''\n",
    "    global g_idx\n",
    "    set_cnt = epoch % rounds\n",
    "    if( (set_cnt) == 0 ) :\n",
    "        #print(\"Reshuffling...\")\n",
    "        g_idx = np.arange(0, labels.shape[0])\n",
    "        np.random.shuffle(g_idx)\n",
    "        \n",
    "\n",
    "    idx = g_idx[set_cnt*num:set_cnt*num+num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_dnn(X_train, y_train, X_test, y_test, batch_size=100, n_epochs=5000) :\n",
    "    \n",
    "    #DNN approach\n",
    "   \n",
    "    tf.reset_default_graph()\n",
    " \n",
    "    # Hyper parameters\n",
    "    fc1_node = 200\n",
    "    beta = 0.01                  # Regularization 0.01\n",
    "    dropout_rate = 0.5           # Dropout rate for dropout layer\n",
    "    starter_learning_rate = 0.001\n",
    "    \n",
    "    # Variables and inputs\n",
    "    X = tf.placeholder(tf.float32, shape=(None, 480), name=\"X\")\n",
    "    input = tf.reshape(X, [-1, 60*8])\n",
    "    y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "    mode = tf.placeholder(tf.bool, shape=(None), name=\"mode\")\n",
    "    scale = tf.placeholder(tf.float32, shape=(None), name=\"scale\")\n",
    "    global_step = tf.placeholder(tf.int32, shape=(None), name=\"global_step\")\n",
    "    learning_rate = tf.Variable(starter_learning_rate, trainable=False)\n",
    "    \n",
    "\n",
    "    # ============ Build the model ============\n",
    "    # Add the fully connected layer\n",
    "    fc1 = tf.layers.dense(\n",
    "        inputs= input, \n",
    "        units=fc1_node, \n",
    "        activation=tf.nn.relu, \n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=scale, scope=None),\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        name=\"fc1\")\n",
    "   \n",
    "    #Adding a dropout layer to avoid overfitting\n",
    "    dropout1 = tf.layers.dropout(inputs=fc1, rate=dropout_rate, training=mode )\n",
    "    \n",
    "    # The last output layer\n",
    "    logits = tf.layers.dense(\n",
    "        inputs=dropout1, \n",
    "        units=3,\n",
    "        activation=None,    #softmax is done in tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=scale, scope=None),\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        name=\"output_FC\")\n",
    "    \n",
    "    # Cross entropy\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    \n",
    "    # Apply regularization in training stage and not in prediction stage\n",
    "    with tf.name_scope('loss'):\n",
    "        loss = tf.reduce_mean(entropy) + beta*sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES))\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        # Decayed learning rate\n",
    "        # Start learning rate as 0.001, decay rate as 0.5, decay 6 steps to ~0.000015 (~1e-5)\n",
    "        learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                                   decay_steps=n_epochs//6, decay_rate=0.5, \n",
    "                                                   staircase=True)\n",
    "        tf.summary.scalar('learning_rate', learning_rate)\n",
    "        # Use Adam optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    with tf.name_scope('accuracy'):\n",
    "        with tf.name_scope('correct_prediction'):\n",
    "            correct_prediction = tf.nn.in_top_k(logits, y, 1)\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    # Merge all the summaries and write them out to /tmp/mnist_logs (by default)\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "    \n",
    "    # ============ Run the session, train and verify ============\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        train_writer = tf.summary.FileWriter(LOGDIR+'train', sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(LOGDIR+'test')\n",
    "        \n",
    "        init.run()\n",
    "        n_rounds = X_train.shape[0] // batch_size\n",
    "        for epoch in range(n_epochs):\n",
    "            X_batch, y_batch = next_batch(batch_size, X_train, y_train, epoch, n_rounds)\n",
    "            summary_train, _ = sess.run([merged, optimizer], \n",
    "                                        feed_dict= {X: X_batch, y: y_batch, \n",
    "                                                    mode: MY_MODE_TRAINING, scale: beta,\n",
    "                                                    global_step: epoch}) \n",
    "            train_writer.add_summary(summary_train, epoch)\n",
    "\n",
    "            if (epoch % 10 == 0):\n",
    "                _, acc_train = sess.run([merged, accuracy], feed_dict={X: X_train, y: y_train, \n",
    "                                                    mode: MY_MODE_PREDICTION, scale: 0,\n",
    "                                                    global_step: epoch})\n",
    "                \n",
    "                summary_test, acc_test = sess.run([merged, accuracy], \n",
    "                                                  feed_dict={X: X_test, y:y_test, \n",
    "                                                            mode: MY_MODE_PREDICTION, scale: 0,\n",
    "                                                            global_step: epoch})\n",
    "                test_writer.add_summary(summary_test, epoch)\n",
    "               \n",
    "                print(epoch, \"Train accuracy:\", acc_train,  \"Test_accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_cnn(X_train, y_train, X_test, y_test, batch_size=100, n_epochs=2000) :\n",
    "    \n",
    "    #CNN approach\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    #Hyper parameters\n",
    "    conv1_depth = 64\n",
    "    conv1_kernel_size = [3, 3]\n",
    "    dense1_node = 64\n",
    "    dense2_node = 16\n",
    "    beta = 0.02                  # Regularization 0.01\n",
    "    dropout_rate = 0.5           # Dropout layer for regularization\n",
    "    starter_learning_rate = 0.001\n",
    "\n",
    "    # Tensor variables and inputs\n",
    "    X = tf.placeholder(tf.float32, shape=(None, 480), name=\"X\")\n",
    "    input = tf.reshape(X, [-1, 60, 8, 1])\n",
    "    y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "    mode = tf.placeholder(tf.bool, shape=(None), name=\"mode\")\n",
    "    scale = tf.placeholder(tf.float32, shape=(None), name=\"scale\")\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.Variable(starter_learning_rate, trainable=False)\n",
    "    \n",
    "\n",
    "    \n",
    "    # ============ Build the model ============\n",
    "    # Convolutional Layer #1\n",
    "    # Computes 32 features using a 3x3 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 60, 8, 1]\n",
    "    # Output Tensor Shape: [batch_size, 60, 8, 64]\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input,\n",
    "        filters=conv1_depth,\n",
    "        kernel_size=conv1_kernel_size,\n",
    "        padding=\"same\",\n",
    "        activation=tf.nn.relu,\n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=scale, scope=None),\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "        name=\"conv1\")\n",
    "\n",
    "    #Adding a pooling layer - Output Tensor Shape: [batch_size, 30, 4, 64]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2], strides=2)\n",
    "    pool_flat = tf.reshape(pool1, [-1, 30 * 4 * conv1_depth])\n",
    "\n",
    "    #Adding a fully connected layer\n",
    "    dense1 = tf.layers.dense(\n",
    "        inputs=pool_flat, \n",
    "        units=dense1_node, \n",
    "        activation=tf.nn.relu, \n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=scale, scope=None),\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        name=\"dense1\")\n",
    "    \n",
    "    #Adding a dropout layer to avoid overfitting\n",
    "    dropout1 = tf.layers.dropout(inputs=dense1, rate=dropout_rate, training=mode )  \n",
    "\n",
    "    # Add the 2nd fully connected layer\n",
    "    dense2 = tf.layers.dense(\n",
    "        inputs=dropout1, \n",
    "        units=dense2_node, \n",
    "        activation=tf.nn.relu, \n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=scale, scope=None),\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        name=\"dense2\")\n",
    "  \n",
    "     # The last output layer\n",
    "    logits = tf.layers.dense(\n",
    "        inputs=dense2, \n",
    "        units=3,\n",
    "        activation=None,    #softmax is done in tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "        kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=scale, scope=None),\n",
    "        kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        name=\"output_FC\")\n",
    "\n",
    "    # Cross entropy\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    \n",
    "    # Apply regularization in training stage and not in prediction stage\n",
    "    reg_losses = sum( tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) )\n",
    "    loss_noreg = tf.reduce_mean(entropy)\n",
    "    loss = loss_noreg + beta * reg_losses\n",
    "    \n",
    "\n",
    "    # Decayed learning rate\n",
    "    # # Start learning rate as 0.001, decay rate as 0.5, decay 6 steps to ~0.000015 (~1e-5)\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                               decay_steps=n_epochs//6, decay_rate=0.5, staircase=True)\n",
    "    \n",
    "    # Use Adam optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)   \n",
    "\n",
    "    correct_prediction = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    \n",
    "\n",
    "    # ============ Run the session, train and verify ============\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        n_rounds = X_train.shape[0] // batch_size\n",
    "        for epoch in range(n_epochs):\n",
    "            X_batch, y_batch = next_batch(batch_size, X_train, y_train, epoch, n_rounds)\n",
    "            sess.run(optimizer, feed_dict= {X: X_batch, y: y_batch, \n",
    "                                            mode: MY_MODE_TRAINING, scale: beta,\n",
    "                                            global_step: epoch}) \n",
    "\n",
    "            if (epoch % 10 == 0):\n",
    "                acc_train = accuracy.eval(feed_dict={X: X_train, y: y_train, \n",
    "                                                     mode: MY_MODE_PREDICTION, scale: 0,\n",
    "                                                     global_step: epoch}) \n",
    "                acc_test = accuracy.eval(feed_dict={X: X_test, y:y_test, \n",
    "                                                    mode: MY_MODE_PREDICTION, scale: 0,\n",
    "                                                    global_step: epoch})  \n",
    "                print(epoch, \"Train accuracy:\", acc_train,  \"Test_accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_rnn(X_train, y_train, X_test, y_test, batch_size=100, n_epochs=2000) :\n",
    "    \n",
    "    #RNN approach\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # Input/output data dimension\n",
    "    n_steps =  60\n",
    "    n_inputs = 8\n",
    "    n_outputs = 3\n",
    "    \n",
    "    #Hyper parameters\n",
    "    n_neurons = 64\n",
    "    beta = 0.1                   # Regularization 0.2\n",
    "    dropout_rate = 0.5           # Dropout layer for regularization\n",
    "    starter_learning_rate = 0.001\n",
    "\n",
    "    # Tensor variables and inputs\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_steps * n_inputs), name=\"X\")\n",
    "    input = tf.reshape(X, [-1, n_steps, n_inputs])\n",
    "    y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "    mode = tf.placeholder(tf.bool, shape=(None), name=\"mode\")\n",
    "    scale = tf.placeholder(tf.float32, shape=(None), name=\"scale\")\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.Variable(starter_learning_rate, trainable=False)\n",
    "    \n",
    "\n",
    "    # ============ Build the model ============\n",
    "    # RNN layer - GRUCell and LSTMCell are both tested, but slow convergence and similar accuracy\n",
    "    basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "    \n",
    "    # Use dynamic_rnn to simulate time step \n",
    "    # Outputs shape [batch, step, n_neurons], state shape: [batch, n_neurons]\n",
    "    outputs, states = tf.nn.dynamic_rnn(basic_cell, input, dtype=tf.float32)\n",
    "    \n",
    "    # Take the output from last timestamp\n",
    "    outputs = tf.transpose(outputs, [1, 0, 2])\n",
    "    rnn_output = tf.gather(outputs, int(outputs.get_shape()[0]) - 1)\n",
    "    \n",
    "    # Dropout layer to avoid overfitting\n",
    "    dropout = tf.layers.dropout(inputs=rnn_output, rate=dropout_rate, training= mode )\n",
    "\n",
    "    # Fully connected layer - Xavier initializer and L2 regularizer\n",
    "    logits = tf.contrib.layers.fully_connected(\n",
    "        inputs=dropout, \n",
    "        num_outputs=n_outputs, \n",
    "        weights_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "        weights_regularizer=tf.contrib.layers.l2_regularizer(scale=beta, scope=None),\n",
    "        activation_fn=None,    #softmax is done in tf.nn.sparse_softmax_cross_entropy_with_logits\n",
    "    )\n",
    "\n",
    "    entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    \n",
    "    # Apply regularization to training stage and not in prediction stage\n",
    "    reg_losses = sum( tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES) )\n",
    "    loss_noreg = tf.reduce_mean(entropy)\n",
    "    loss = loss_noreg + beta * reg_losses\n",
    "\n",
    "    # Decayed learning rate\n",
    "    # # Start learning rate as 0.001, decay rate as 0.5, decay 6 steps to ~0.000015 (~1e-5)\n",
    "    learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                               decay_steps=n_epochs//6, decay_rate=0.5, \n",
    "                                               staircase=True)\n",
    "    \n",
    "    # Adam optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    \n",
    "    # ============ Run the session, train and verify ============\n",
    "    init = tf.global_variables_initializer()\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        n_rounds = X_train.shape[0] // batch_size\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            X_batch, y_batch = next_batch(batch_size, X_train, y_train, epoch, n_rounds)\n",
    "            sess.run(optimizer, feed_dict= {X: X_batch, y: y_batch, mode: MY_MODE_TRAINING, \n",
    "                                            scale: beta, global_step: epoch}) \n",
    "\n",
    "            if (epoch % 10 == 0):\n",
    "                acc_train = accuracy.eval( feed_dict={X: X_train, y: y_train, \n",
    "                                                      mode: MY_MODE_PREDICTION, scale: 0, \n",
    "                                                      global_step: epoch}) \n",
    "                acc_test = accuracy.eval( feed_dict={X: X_test, y:y_test, \n",
    "                                                     mode: MY_MODE_PREDICTION, scale: 0, \n",
    "                                                     global_step: epoch})  \n",
    "                print(epoch, \"Train accuracy:\", acc_train,  \"Test_accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "    \n",
    "    # Load data\n",
    "    df = load_data(\"..\\data\\splice.data\")\n",
    "    \n",
    "    # Preprocess data and split training and validation data\n",
    "    X_train, y_train, X_test, y_test = preprocess_data(df)\n",
    "    \n",
    "    # DNN model\n",
    "    #model_dnn(X_train, y_train, X_test, y_test, batch_size=100, n_epochs=1000)\n",
    "        \n",
    "    # CNN model\n",
    "    model_cnn(X_train, y_train, X_test, y_test, batch_size=100, n_epochs=1000)\n",
    "    \n",
    "    # RNN model\n",
    "    #model_rnn(X_train, y_train, X_test, y_test, batch_size=100, n_epochs=2000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3190\n",
      "Training samples:  2552 Test samples:  638\n",
      "0 Train accuracy: 0.518809 Test_accuracy: 0.518809\n",
      "10 Train accuracy: 0.518809 Test_accuracy: 0.518809\n",
      "20 Train accuracy: 0.538401 Test_accuracy: 0.534483\n",
      "30 Train accuracy: 0.567006 Test_accuracy: 0.556426\n",
      "40 Train accuracy: 0.689263 Test_accuracy: 0.675549\n",
      "50 Train accuracy: 0.873824 Test_accuracy: 0.84953\n",
      "60 Train accuracy: 0.880878 Test_accuracy: 0.868339\n",
      "70 Train accuracy: 0.920455 Test_accuracy: 0.884013\n",
      "80 Train accuracy: 0.923198 Test_accuracy: 0.89185\n",
      "90 Train accuracy: 0.92163 Test_accuracy: 0.887147\n",
      "100 Train accuracy: 0.940831 Test_accuracy: 0.905956\n",
      "110 Train accuracy: 0.934169 Test_accuracy: 0.902821\n",
      "120 Train accuracy: 0.945925 Test_accuracy: 0.920063\n",
      "130 Train accuracy: 0.959639 Test_accuracy: 0.931035\n",
      "140 Train accuracy: 0.964734 Test_accuracy: 0.937304\n",
      "150 Train accuracy: 0.966693 Test_accuracy: 0.943574\n",
      "160 Train accuracy: 0.971003 Test_accuracy: 0.945141\n",
      "170 Train accuracy: 0.973354 Test_accuracy: 0.948276\n",
      "180 Train accuracy: 0.975705 Test_accuracy: 0.946708\n",
      "190 Train accuracy: 0.972571 Test_accuracy: 0.945141\n",
      "200 Train accuracy: 0.971003 Test_accuracy: 0.946708\n",
      "210 Train accuracy: 0.978056 Test_accuracy: 0.951411\n",
      "220 Train accuracy: 0.978056 Test_accuracy: 0.951411\n",
      "230 Train accuracy: 0.980799 Test_accuracy: 0.952978\n",
      "240 Train accuracy: 0.978056 Test_accuracy: 0.949843\n",
      "250 Train accuracy: 0.977273 Test_accuracy: 0.951411\n",
      "260 Train accuracy: 0.97884 Test_accuracy: 0.948276\n",
      "270 Train accuracy: 0.981975 Test_accuracy: 0.951411\n",
      "280 Train accuracy: 0.981975 Test_accuracy: 0.960815\n",
      "290 Train accuracy: 0.984326 Test_accuracy: 0.951411\n",
      "300 Train accuracy: 0.985893 Test_accuracy: 0.960815\n",
      "310 Train accuracy: 0.986285 Test_accuracy: 0.954545\n",
      "320 Train accuracy: 0.98942 Test_accuracy: 0.954545\n",
      "330 Train accuracy: 0.98942 Test_accuracy: 0.959248\n",
      "340 Train accuracy: 0.988636 Test_accuracy: 0.965517\n",
      "350 Train accuracy: 0.989812 Test_accuracy: 0.965517\n",
      "360 Train accuracy: 0.990596 Test_accuracy: 0.959248\n",
      "370 Train accuracy: 0.98942 Test_accuracy: 0.967085\n",
      "380 Train accuracy: 0.990987 Test_accuracy: 0.959248\n",
      "390 Train accuracy: 0.991771 Test_accuracy: 0.96395\n",
      "400 Train accuracy: 0.991379 Test_accuracy: 0.959248\n",
      "410 Train accuracy: 0.98942 Test_accuracy: 0.95768\n",
      "420 Train accuracy: 0.988245 Test_accuracy: 0.970219\n",
      "430 Train accuracy: 0.991379 Test_accuracy: 0.965517\n",
      "440 Train accuracy: 0.991379 Test_accuracy: 0.956113\n",
      "450 Train accuracy: 0.991771 Test_accuracy: 0.962382\n",
      "460 Train accuracy: 0.990987 Test_accuracy: 0.96395\n",
      "470 Train accuracy: 0.990987 Test_accuracy: 0.956113\n",
      "480 Train accuracy: 0.992163 Test_accuracy: 0.967085\n",
      "490 Train accuracy: 0.992555 Test_accuracy: 0.962382\n",
      "500 Train accuracy: 0.993339 Test_accuracy: 0.962382\n",
      "510 Train accuracy: 0.992555 Test_accuracy: 0.95768\n",
      "520 Train accuracy: 0.992947 Test_accuracy: 0.960815\n",
      "530 Train accuracy: 0.992947 Test_accuracy: 0.960815\n",
      "540 Train accuracy: 0.99373 Test_accuracy: 0.962382\n",
      "550 Train accuracy: 0.993339 Test_accuracy: 0.96395\n",
      "560 Train accuracy: 0.992947 Test_accuracy: 0.959248\n",
      "570 Train accuracy: 0.993339 Test_accuracy: 0.96395\n",
      "580 Train accuracy: 0.99373 Test_accuracy: 0.96395\n",
      "590 Train accuracy: 0.993339 Test_accuracy: 0.962382\n",
      "600 Train accuracy: 0.992947 Test_accuracy: 0.96395\n",
      "610 Train accuracy: 0.994514 Test_accuracy: 0.962382\n",
      "620 Train accuracy: 0.99373 Test_accuracy: 0.96395\n",
      "630 Train accuracy: 0.99373 Test_accuracy: 0.960815\n",
      "640 Train accuracy: 0.994906 Test_accuracy: 0.959248\n",
      "650 Train accuracy: 0.994906 Test_accuracy: 0.96395\n",
      "660 Train accuracy: 0.99373 Test_accuracy: 0.962382\n",
      "670 Train accuracy: 0.994906 Test_accuracy: 0.96395\n",
      "680 Train accuracy: 0.994906 Test_accuracy: 0.962382\n",
      "690 Train accuracy: 0.994514 Test_accuracy: 0.96395\n",
      "700 Train accuracy: 0.994122 Test_accuracy: 0.96395\n",
      "710 Train accuracy: 0.994122 Test_accuracy: 0.96395\n",
      "720 Train accuracy: 0.99373 Test_accuracy: 0.962382\n",
      "730 Train accuracy: 0.994906 Test_accuracy: 0.96395\n",
      "740 Train accuracy: 0.994514 Test_accuracy: 0.96395\n",
      "750 Train accuracy: 0.994514 Test_accuracy: 0.96395\n",
      "760 Train accuracy: 0.994514 Test_accuracy: 0.962382\n",
      "770 Train accuracy: 0.996082 Test_accuracy: 0.96395\n",
      "780 Train accuracy: 0.995298 Test_accuracy: 0.96395\n",
      "790 Train accuracy: 0.995298 Test_accuracy: 0.96395\n",
      "800 Train accuracy: 0.994514 Test_accuracy: 0.960815\n",
      "810 Train accuracy: 0.994514 Test_accuracy: 0.960815\n",
      "820 Train accuracy: 0.995298 Test_accuracy: 0.965517\n",
      "830 Train accuracy: 0.99569 Test_accuracy: 0.965517\n",
      "840 Train accuracy: 0.996082 Test_accuracy: 0.965517\n",
      "850 Train accuracy: 0.996082 Test_accuracy: 0.96395\n",
      "860 Train accuracy: 0.995298 Test_accuracy: 0.96395\n",
      "870 Train accuracy: 0.994906 Test_accuracy: 0.96395\n",
      "880 Train accuracy: 0.99569 Test_accuracy: 0.965517\n",
      "890 Train accuracy: 0.996473 Test_accuracy: 0.96395\n",
      "900 Train accuracy: 0.99569 Test_accuracy: 0.96395\n",
      "910 Train accuracy: 0.99569 Test_accuracy: 0.962382\n",
      "920 Train accuracy: 0.995298 Test_accuracy: 0.96395\n",
      "930 Train accuracy: 0.996473 Test_accuracy: 0.965517\n",
      "940 Train accuracy: 0.99569 Test_accuracy: 0.965517\n",
      "950 Train accuracy: 0.99569 Test_accuracy: 0.96395\n",
      "960 Train accuracy: 0.99569 Test_accuracy: 0.96395\n",
      "970 Train accuracy: 0.996082 Test_accuracy: 0.965517\n",
      "980 Train accuracy: 0.99569 Test_accuracy: 0.96395\n",
      "990 Train accuracy: 0.99569 Test_accuracy: 0.96395\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\qli45\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
